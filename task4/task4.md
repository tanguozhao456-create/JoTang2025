## 概念理解
从 RNN、LSTM、GRU 中选择两个，介绍其基本原理和优缺点。  
RNN：专门为处理序列数据（比如时间序列、语音、文本）而设计的神经网络。  
基本原理：
RNN沿着输入和输出序列的符号位置分解计算。将位置与计算时间中的步骤对齐，
生成一个隐藏状态序列ht，作为先前隐藏状态ht−1和位置t输入的函数。

优缺点：  
①​序列建模能力强，是处理序列数据的天然结构，能够捕捉时间上的依赖性；不同时间步共享参数，模型效率高，具有一定的泛化能力；可以处理多种输入输出模式，有一对一、多对多、多对一、一对多等结构  
②存在​梯度消失和梯度爆炸问题​，在处理长序列时，梯度需要通过时间反向传播。当网络较深（时间步很长）时，梯度连乘会导致梯度变得极小（消失）或极大（爆炸），使得模型无法学习到长距离的依赖关系。且由于梯度消失，RNN会遗忘序列中较早的信息，只能拥有“短期记忆”。

LSTM:  
基本原理：  

优缺点：  
①优点：在长依赖问题比RNN性能刚好​，通过门控机制和细胞状态，LSTM能够有效地学习和记忆长序列中的长期依赖关系；在众多序列建模任务上性能较好，比如机器翻译、文本生成、语音识别。  
②缺点：计算更复杂，结构比RNN复杂得多，参数数量是标准RNN的四倍，因此训练和推理速度较慢，计算资源消耗大。  

GRU:  
基本原理：  

优缺点：  
①优点：参数比LSTM少（只有两个门，三个权重矩阵），因此训练速度更快，计算开销更小；性能与LSTM相当甚至更好  
②缺点：表达能力可能稍弱，对于某些非常复杂的、需要精细长期记忆的任务，GRU优势可能不如LSTM

### Transformer

注意力机制的分类？现在有哪些最常用？  
笔记：注意力机制是一种允许模型在处理信息时专注于关键部分，忽略不相关信息，从而提高处理效率和准确性的机制。有利于解决RNN和LSTM等传统的序列处理模型的长依赖问题。

什么是词向量？为什么我们需要用向量来表示词？  
①词向量是词嵌入式自然语言处理（NLP）中的一组语言建模和特征学习技术的统称，其中来自词汇表的单词或短语被映射到实数的向量。
②​计算机无法直接理解文字和词语，只能理解以0和1组成的二进制编码，所以需要将文字和词语转化成数字向量，以此在计算机中表示文字和词语

one-hot 编码是什么？为什么使用 one-hot 编码来表示词语会导致维度灾难？  
①One-Hot 编码是一种将分类数据转换为数值数据的方法，有N个数据则建立N维向量，对于某个具体数据，只有对应取值那一位为1，其他为0
②向量的维度和文本中的词语数量相关，例子中的单词数量为4，所以我们的向量表示为1*4。但是在自然语言处理中，往往需要大量的数据，如果词库中存在10万的单词量，那每个单词的向量就表现为10万维。向量的维度过高，会导致模型的训练难度增加，难以收敛。

Word2Vec 提供了哪两种训练架构？分别是如何工作的？  
笔记：Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，通过学习文本来用词向量的方式表征词的语义信息，即语义上相似的单词在该向量空间内距离很近。  
Word2Vec 的两种训练架构： 
工作方式不同 
①CBOW：
通过上下文词，推理中心词。在上下文已知的情况下，预测中心词出现的概率。使用上下文的词向量推理中心词，中心词的语义就被传递到上下文的词向量中。
②Skip-Gram：
与CBOW的训练方式相反，Skip-Gram是通过中心词来预测上下文词。在中心词已知的情况下，预测上下文词出现的概率。
训练方式相同：
首先，构建神经网络，输入为句子中的某个词（中心词），输出为其他所有词出现在中心词周围的概率。在输出时，输出的词既有和中心词相邻的词或者是在中心词上下文中附近出现过的词（相关词），也会有不相关的各种词。  
其次，在训练时，我们希望输出的词就是围绕在中心词周围的词，因为输出词的概率乘积越大，就说明输出词和中心词越相关。
最后，通过梯度下降法使得输出词的概率乘积尽可能地大，最终在不断的更新下，就得到了所有词的词向量。

### 位置编码

为什么Transformer 本身没有任何位置意识？
Transformer 在计算注意力时只关注词与词之间的相似度，而不是词在句子中的位置顺序。词与词在向量中的位置只代表词义相似度，不代表顺序。

位置编码可以是 绝对的（Absolute）或 相对的（Relative），还可以是可学习的（Learnable），他们分别是什么、有什么优劣？  
绝对位置编码：  
正余弦编码，为序列中的每个绝对位置（第1个词，第2个词，...）分配一个唯一的编码向量。  
优势：允许模型通过线性运算轻松学习到相对位置，从而为长度外推提供了数学便利，在理论上具备良好的长度外推能力。
劣势：无法最优捕捉语言中的相对关系，对长序列的泛化能力有限

相对位置编码：  
​T5式相对偏置​：在注意力分数中加入可学习的相对位置偏置  
​经典相对编码​：为每个相对距离学习一个嵌入向量  
用向量元素的距离表示词与词语义的相似度  
优势：更符合语言学的直觉，因为语法关系多由相对顺序决定；通常有更好的泛化能力；能更好地处理长距离依赖  
劣势：实现复杂，计算开销较大；需要设计合理的相对距离范围

可学习编码：  
将位置编码视为可训练参数，通过梯度下降学习得到。  
优势：极其简单，易于实现；数据驱动，可能学到最优的位置表示；在固定长度任务上表现良好  
劣势：无法外推，严格限制在训练时设定的最大长度；可能过拟合训练数据的特定位置模式  

大语言模型（LLM）中常用的位置编码有哪些？  
旋转位置编码（RoPE）、ALiBi、T5式相对偏置、Transformer-XL的相对编码

### 层归一化 LayerNorm

什么是 Layer Normalization？为什么需要它？  
①Layer Normalization（层归一化）​​ 是一种深度学习中的归一化技术，旨在解决深度神经网络训练过程中的内部协变量偏移​问题。在同一个样本内部，对某一神经网络层所有输出神经元（即该层的激活值）进行标准化处理。首先求输入向量z的所有特征值​的平均数和方差，然后对向量进行标准化，最后对标准化后的数据施加一个可学习的缩放和偏移。  
②

LayerNorm 和 BatchNorm 的区别？
LayerNorm是对同一个样本的在同一层所有特征值输出进行标准化  
BatchNorm是对同一个特征在同一层所有样本的特征值输出进行标准化

### mask

什么是 Causal Mask / Look-ahead Mask？作用是什么？  
①Causal Mask是一种特殊的注意力掩码，用于确保在序列生成过程中，每个位置只能关注到它之前的位置（包括自己），而不能关注到它之后的位置。  
②在训练自回归语言模型并使用教师强制时，让模型只能看到前一个词，模拟了真实推理的情况，确保模型不会作弊。

### 束搜索

为什么 LLM 输出序列是一个搜索问题（每次输出一个概率分布列）？
LLM 是自回归模型，通过迭代预测下一个词来生成序列，预测通过概率来实现。这个过程本质上就是一个序列决策问题，而序列决策问题天然就是一个搜索问题。

### BERT

解释 BERT 的架构和原理？  
①BERT基于Transformer架构，但只使用Transformer的编码器部分。
②将输入文本转换为三个嵌入向量的和，然后通过多个Transformer编码器层进行深度双向编码，其中每个词的表征都融入了整个输入序列的上下文信息，输出每个输入位置的上下文相关向量表示

为什么说 BERT 是“双向的”？  
BERT通过多层自注意力，实现词与词之间的深度双向交互.在计算每个词的表示时，Transformer的自注意力机制允许BERT同时关注输入序列中的所有词，同时看到整个上下文​；

BERT 的预训练任务是什么？  
MLM掩码语言建模和NSP下一句预测。前者具体为随机遮盖输入中15%的词，其中：80%用掩码替换，10%用随机词替换，10%保持不变，然后让模型预测被遮盖的原始词；后者具体为输入两个句子A和B，预测B是否是A的下一句，其中50%情况下B是A的真实下一句，50%情况下是随机句子

BERT 的输入嵌入（Input Embedding）是由哪三部分相加而成的？为什么需要这三部分？
①词嵌入、段嵌入和位置嵌入
②词嵌入确保基础的语义表示，段嵌入确定句子的边界信息，位置嵌入确定序列的顺序信息

BERT 和 Transformer 有何区别？
BERT是基于Transformer编码器的预训练模型，仅使用Transformer的编码器部分，用于预训练模型；Transformer是一个完整的序列到序列架构，包含完整的编码器-解码器，专为机器翻译等序列生成任务设计
